{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "# load users\n",
    "with open(\"users.csv\") as f:\n",
    "    users = f.read().splitlines() \n",
    "    users = users[1:]\n",
    "# load train data\n",
    "with open(\"train.csv\") as f:\n",
    "    links = f.read().splitlines() \n",
    "    links = links[1:]\n",
    "# load test data\n",
    "with open(\"test.csv\") as f:\n",
    "    tests = f.read().splitlines() \n",
    "    tests = tests[1:]\n",
    "# taking a sample of the data\n",
    "#train data sample\n",
    "links_sample = random.sample(links, 20000)\n",
    "#adjacancy matrix user sample\n",
    "users_sample = random.sample(users, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 801211.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# captturing nodes in 2 separate lists\n",
    "node_list_1 = []\n",
    "node_list_2 = []\n",
    "for i in tqdm(links_sample):\n",
    "    node_list_1.append(i.split(',')[0])\n",
    "    node_list_2.append(i.split(',')[1])\n",
    "#creating the data frame\n",
    "df = pd.DataFrame({'node_1': node_list_1, 'node_2': node_list_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the graph\n",
    "\n",
    "G = nx.from_pandas_edgelist(df, \"node_1\", \"node_2\", create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all nodes in a list to idetify the users\n",
    "node_list = node_list_1 + node_list_2\n",
    "# remove duplicate items from the list\n",
    "node_list = list(dict.fromkeys(node_list))\n",
    "# build adjacency matrix\n",
    "adj_G = nx.to_numpy_matrix(G, nodelist = node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17553/17553 [05:17<00:00, 55.24it/s]  \n"
     ]
    }
   ],
   "source": [
    "# get unconnected node-pairs\n",
    "all_unconnected_pairs = []\n",
    "\n",
    "# traverse adjacency matrix\n",
    "offset = 0\n",
    "for i in tqdm(range(adj_G.shape[0])):\n",
    "    for j in range(offset,adj_G.shape[1]):\n",
    "        if i != j and j > i:\n",
    "            if adj_G[i,j] == 0:\n",
    "                all_unconnected_pairs.append([node_list[i],node_list[j]])\n",
    "    offset = offset + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating a data frame to train the model with it\n",
    "#choosing only 30,000 negative data\n",
    "all_unconnected_pairs = random.sample(all_unconnected_pairs, 30000)\n",
    "node_1_unlinked = [i[0] for i in all_unconnected_pairs]\n",
    "node_2_unlinked = [i[1] for i in all_unconnected_pairs]\n",
    "\n",
    "data = pd.DataFrame({'node_1':node_1_unlinked, \n",
    "                     'node_2':node_2_unlinked})\n",
    "\n",
    "# add target variable 'link'\n",
    "data['link'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding positive data to the training data\n",
    "df_ghost = df\n",
    "\n",
    "# add the target variable 'link'\n",
    "df_ghost['link'] = 1\n",
    "\n",
    "data = data.append(df_ghost[['node_1', 'node_2', 'link']], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30000\n",
       "1    20000\n",
       "Name: link, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the count of each value. \n",
    "data['link'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a graph with the training data\n",
    "G_data = nx.from_pandas_edgelist(df, \"node_1\", \"node_2\", create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 17553/17553 [00:05<00:00, 3027.13it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:52<00:00,  5.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "# Generate walks\n",
    "node2vec = Node2Vec(G_data, dimensions=100, walk_length=8, num_walks=10)\n",
    "\n",
    "# train node2vec model\n",
    "n2w_model = node2vec.fit(window=7, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "x = [(n2w_model[str(i)]+n2w_model[str(j)]) for i,j in zip(data['node_1'], data['node_2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = np.array(x)\n",
    "ytrain = data['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating and training the model\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "lr.fit(xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 675711.59it/s]\n"
     ]
    }
   ],
   "source": [
    "#selecting 50,000 test data\n",
    "tests = random.sample(tests, 50000)\n",
    "\n",
    "# captture nodes in 2 separate lists\n",
    "node_list_1 = []\n",
    "node_list_2 = []\n",
    "node_list_3 = []\n",
    "\n",
    "# creating data frame\n",
    "for i in tqdm(tests):\n",
    "    node_list_1.append(i.split(',')[0])\n",
    "    node_list_2.append(i.split(',')[1])\n",
    "    node_list_3.append(i.split(',')[2])\n",
    "\n",
    "test_df = pd.DataFrame({'node_1':node_list_1, \n",
    "                     'node_2':node_list_2})\n",
    "test_data = test_df\n",
    "test_data['link'] = node_list_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_data = nx.from_pandas_edgelist(test_df, \"node_1\", \"node_2\", create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 62287/62287 [00:25<00:00, 2397.73it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [04:20<00:00, 26.09s/it]\n"
     ]
    }
   ],
   "source": [
    "#feature selection\n",
    "# Generate walks\n",
    "node2vec = Node2Vec(T_data, dimensions=100, walk_length=8, num_walks=10)\n",
    "\n",
    "n2w_model = node2vec.fit(window=7, min_count=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "x_test = [(n2w_model[str(i)]+n2w_model[str(j)]) for i,j in zip(test_data['node_1'], test_data['node_2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.array(x_test)\n",
    "ytest = []\n",
    "for i in test_data['link']:\n",
    "    ytest.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "predictions = lr.predict_proba(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rounding the probabilities to 0 and 1\n",
    "rounded_predictions = []\n",
    "for i in predictions[:,1]:\n",
    "    if i > 0.5:\n",
    "        rounded_predictions.append(int(1))\n",
    "    else:\n",
    "        rounded_predictions.append(int(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5198271291519619"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating recall\n",
    "recall_score(ytest, rounded_predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8978167898464231"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating precision\n",
    "precision_score(ytest, rounded_predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4944454390404074"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating f1 \n",
    "f1_score(ytest, rounded_predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
